{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../MultiOmicsGraphEmbedding/\")\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"wandb\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Necessary imports\n",
    "import pickle, os, time, random, datetime, itertools, warnings\n",
    "from argparse import Namespace\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "from umap import UMAP\n",
    "from openomics import MultiOmics\n",
    "from openomics.database import GeneOntology\n",
    "from moge.network.heterogeneous import HeterogeneousNetwork\n",
    "from moge.visualization.data import heatmap, plot_training_history, heatmap_compare, clf_report, clf_report_compare\n",
    "from moge.visualization.network import graph_viz, graph_viz3d\n",
    "\n",
    "from moge.evaluation.utils import largest_indices\n",
    "from moge.evaluation.embedding import distances_correlation\n",
    "from moge.visualization.evaluation import plot_roc_curve_multiclass, plot_roc_curve, plot_pr_curve_multiclass\n",
    "from moge.generator import SubgraphGenerator, SubgraphDataset\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch_geometric import datasets\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import loggers\n",
    "\n",
    "from torchsample.callbacks import ReduceLROnPlateau\n",
    "from torchsample.regularizers import L1Regularizer, L2Regularizer\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.classification.transformer import Transformer, StarEncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18815 entries, A1BG to ZZEF1\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   gene_name             18815 non-null  object\n",
      " 1   protein_size          17965 non-null  object\n",
      " 2   protein_id            17965 non-null  object\n",
      " 3   annotation            17965 non-null  object\n",
      " 4   Transcript sequence   17965 non-null  object\n",
      " 5   go_id                 18815 non-null  object\n",
      " 6   disease_associations  8755 non-null   object\n",
      " 7   omic                  18815 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "with open('../MultiOmicsGraphEmbedding/moge/data/gtex_string_network.pickle', 'rb') as file:\n",
    "    network = pickle.load(file)\n",
    "\n",
    "network.multiomics.Protein.annotation_expressions = network.multiomics.Protein.expressions.T\n",
    "network.annotations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17965.000000\n",
       "mean       582.315001\n",
       "std        608.918757\n",
       "min         24.000000\n",
       "25%        278.000000\n",
       "50%        437.000000\n",
       "75%        699.000000\n",
       "max      35991.000000\n",
       "Name: Transcript sequence, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.annotations[\"Transcript sequence\"].map(lambda x: len(x) if isinstance(x, str) else None).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT PARAMETERS\n",
    "variables = []\n",
    "# variables = ['chromosome_name', 'transcript_start', 'transcript_end']\n",
    "targets = ['go_id']\n",
    "\n",
    "network.process_feature_tranformer(filter_label=targets[0], min_count=100, verbose=False)\n",
    "classes = network.feature_transformer[targets[0]].classes_\n",
    "n_classes = len(classes)\n",
    "\n",
    "test_frac = 0.05\n",
    "max_length = 1000\n",
    "input_shape = (None, )\n",
    "batch_size = 2000\n",
    "n_steps = int(400000/batch_size)\n",
    "\n",
    "directed = False\n",
    "\n",
    "seed = random.randint(0,1000)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_list 17067 {'directed': 17067, 'undirected': 17067}\n",
      "node_list 898 {'directed': 898, 'undirected': 898}\n"
     ]
    }
   ],
   "source": [
    "split_idx = 0\n",
    "dataset_train = network.get_train_generator(\n",
    "    SubgraphGenerator, split_idx=split_idx, variables=variables, targets=targets,\n",
    "    traversal=\"bfs\", batch_size=batch_size, agg_mode=None,\n",
    "    method=\"GAT\", adj_output=\"coo\",\n",
    "    sampling=\"cycle\", n_steps=n_steps, directed=directed,\n",
    "    maxlen=max_length, padding='post', truncating='post', variable_length=False,\n",
    "    seed=seed, verbose=True)\n",
    "\n",
    "dataset_test = network.get_test_generator(\n",
    "    SubgraphGenerator, split_idx=split_idx, variables=variables, targets=targets,\n",
    "    traversal='all', batch_size=batch_size, agg_mode=None,\n",
    "    method=\"GAT\", adj_output=\"coo\",\n",
    "    sampling=\"log\", n_steps=1, directed=directed,\n",
    "    maxlen=max_length, padding='post', truncating='post', variable_length=False,\n",
    "    seed=seed, verbose=True)\n",
    "\n",
    "dataset_train.tokenizer.word_index == dataset_test.tokenizer.word_index\n",
    "vocab = dataset_train.tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = network.feature_transformer[\"go_id\"].transform(network.annotations[\"go_id\"].str.split(\"|\")).sum(0)\n",
    "# class_weights = 1/class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y, idx_train = dataset_train.__getitem__()\n",
    "# print({k: v.shape if not isinstance(v, list) else (len(v), len(v[0])) for k, v in X_train.items()}, \n",
    "#       {\"y_train\": y.shape}, idx_train.shape)\n",
    "\n",
    "# X, y, idx_train = dataset_test.__getitem__()\n",
    "# print({k: v.shape if not isinstance(v, list) else (len(v), len(v[0])) for k, v in X.items()}, \n",
    "#       {\"y_train\": y.shape}, idx_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def padding_tensor(sequences):\n",
    "#     num = len(sequences)\n",
    "#     max_len = max([s.size(-1) for s in sequences])\n",
    "#     out_dims = (num, 2, max_len)\n",
    "#     out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
    "# #     mask = sequences[0].data.new(*out_dims).fill_(0)\n",
    "#     for i, tensor in enumerate(sequences):\n",
    "#         length = tensor.size(-1)\n",
    "#         out_tensor[i, :, :length] = tensor\n",
    "# #         mask[i, :length] = 1\n",
    "#     return out_tensor\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_seqs_all, subnetwork_all, y_all, idx_all = [], [], [] ,[]\n",
    "#     for X, y, idx in batch:\n",
    "#         input_seqs_all.append(torch.tensor(X[\"input_seqs\"]))\n",
    "#         subnetwork_all.append(torch.tensor(X[\"subnetwork\"]))\n",
    "#         y_all.append(torch.tensor(y))\n",
    "#         idx_all.append(torch.tensor(idx))\n",
    "        \n",
    "#     X_all = {\"input_seqs\": torch.cat(input_seqs_all), \"subnetwork\": padding_tensor(subnetwork_all)}\n",
    "#     return X_all, torch.cat(y_all), torch.cat(idx_all)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': None,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 10,\n",
    "#     'collate_fn': collate_fn,\n",
    "}\n",
    "\n",
    "dataloader_train = data.DataLoader(dataset_train, **params)\n",
    "dataloader_test = data.DataLoader(dataset_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_seqs': torch.Size([2000, 1000]), 'subnetwork': torch.Size([2, 34516])} {'y_train': torch.Size([2000, 2522])} torch.Size([2000])\n",
      "{'input_seqs': torch.Size([898, 1000]), 'subnetwork': torch.Size([2, 2164])} {'y_train': torch.Size([898, 2522])} torch.Size([898])\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "X_train, y, idx_train = next(iter(dataloader_train))\n",
    "print({k: v.shape if not isinstance(v, list) else (len(v), len(v[0])) for k, v in X_train.items()}, \n",
    "      {\"y_train\": y.shape}, idx_train.shape)\n",
    "\n",
    "X, y, idx_train = next(iter(dataloader_test))\n",
    "print({k: v.shape if not isinstance(v, list) else (len(v), len(v[0])) for k, v in X.items()}, \n",
    "      {\"y_train\": y.shape}, idx_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.5.0 available.\n",
      "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Output of `_classifier` is logits\n"
     ]
    }
   ],
   "source": [
    "from moge.module.trainer import LightningModel\n",
    "from moge.module.classifier import EncoderEmbedderClassifier\n",
    "\n",
    "# CUDA for PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "max_epochs = 100\n",
    "\n",
    "hparams = {\n",
    "    \"encoder\": \"ConvLSTM\",\n",
    "    \"encoding_dim\": 128,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"word_embedding_size\": 18,\n",
    "    \"max_length\": max_length,\n",
    "    \n",
    "#     \"num_hidden_layers\": 1,\n",
    "#     \"num_hidden_groups\": 1,\n",
    "#     \"hidden_dropout_prob\": 0.16,\n",
    "#     \"attention_probs_dropout_prob\": 0.1356,\n",
    "#     \"num_attention_heads\": 8,\n",
    "#     \"intermediate_size\": 1024,\n",
    "\n",
    "    \"nb_conv1_filters\": 154,\n",
    "    \"nb_conv1_kernel_size\": 9,\n",
    "    \"nb_conv1_dropout\": 0.4838,\n",
    "    \"nb_conv1_batchnorm\": False,\n",
    "    \n",
    "    \"nb_conv2_filters\": 43,\n",
    "    \"nb_conv2_kernel_size\": 6,\n",
    "    \"nb_conv2_batchnorm\": True,\n",
    "\n",
    "    \"nb_max_pool_size\": 19,\n",
    "\n",
    "    \"nb_lstm_bidirectional\": False,\n",
    "    \"nb_lstm_units\": 162,\n",
    "    \"nb_lstm_hidden_dropout\": 0.1615,\n",
    "    \"nb_lstm_layernorm\": True,\n",
    "\n",
    "    \"embedder\": \"GAT\",\n",
    "    \"embedding_dim\": 256,\n",
    "    \"nb_attn_heads\": 8,\n",
    "    \"nb_attn_dropout\": 0.32,\n",
    "\n",
    "    \"classifier\": \"Dense\",\n",
    "    \"nb_cls_dense_size\": 1536,\n",
    "    \"nb_cls_dropout\": 0.3245,\n",
    "    \"n_classes\": n_classes,\n",
    "\n",
    "    \"nb_weight_decay\": 0.03,\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    \"loss_type\": \"SIGMOID_FOCAL_CROSS_ENTROPY\",\n",
    "    \n",
    "    \"optimizer\": \"adam\",\n",
    "#     \"class_weights\": class_weights,\n",
    "}\n",
    "\n",
    "eec = EncoderEmbedderClassifier(Namespace(**hparams))\n",
    "model = LightningModel(eec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = eec.forward(X_train)\n",
    "# loss = eec.loss(y_hat, y, idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regularizers = [L1Regularizer(scale=1e-3, module_filter='conv*'),\n",
    "                L2Regularizer(scale=1e-5, module_filter='fc*')]\n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"multiplex-rna-embedding\")\n",
    "# wandb_logger.log_hyperparams(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning:Using 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    gpus=1,\n",
    "#     distributed_backend='ddp',\n",
    "    min_epochs=20,\n",
    "    max_epochs=max_epochs,\n",
    "#     early_stop_callback=EarlyStopping(monitor='val_loss', patience=3),\n",
    "#     regularizers=regularizers,\n",
    "#     logger=wandb_logger,\n",
    "    weights_summary='top',\n",
    "    amp_level='O1', precision=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "  | Name   | Type                      | Params\n",
      "-------------------------------------------------\n",
      "0 | _model | EncoderEmbedderClassifier | 4 M   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c8883ed47f416abca26bcbb6be3e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_precision': 0.03791998211523064, 'val_recall': 0.4929057625208655, 'val_top_k': 0.029660927957309645, 'val_loss': 0.12888985872268677}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3132c2975643c2a27ed7d56b8305e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_precision': 0.0, 'val_recall': 0.0, 'val_top_k': 0.30427948881629313, 'val_loss': 0.12596221268177032}\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader=dataloader_train, val_dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wandb_logger.experiment._summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, idx_train = dataset_train.load_data(dropna=True)\n",
    "print({k: v.shape for k, v in X_train.items()}, {\"y_train\": y_train.shape})\n",
    "X_test, y_test, idx_test = dataset_test.load_data(dropna=True)\n",
    "print({k: v.shape for k, v in X_test.items()}, {\"y_test\": y_test.shape})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_test = eec.get_embeddings(X_test, cuda=True)\n",
    "y_test_pred = eec.predict(emb_test, cuda=True)\n",
    "\n",
    "emb_train = eec.cpu().get_embeddings(X_train, cuda=False)\n",
    "y_train_pred = eec.predict(emb_train, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = pd.DataFrame(y_train_pred, index=y_train.index, columns=y_train.columns)\n",
    "y_test_pred = pd.DataFrame(y_test_pred, index=y_test.index, columns=y_test.columns)\n",
    "\n",
    "emb_all = np.concatenate([emb_train, emb_test], axis=0)\n",
    "y_all = np.concatenate([y_train, y_test], axis=0)\n",
    "idx_all = pd.concat([pd.Series([\"train\"] * idx_train.shape[0], index=y_train.index), \n",
    "                     pd.Series([\"test\"] * idx_test.shape[0], index=y_test.index)])\n",
    "y_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test.sum(axis=1).mean(), (y_test_pred>0.5).sum(axis=1).mean()), (y_train.sum(axis=1).mean(), (y_train_pred>0.5).sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_report_compare(y_train, y_train_pred, \n",
    "                   y_test, y_test_pred, \n",
    "                   classes=network.feature_transformer[targets[0]].classes_, \n",
    "                   threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_classes = y_test.sum(0).sort_values(ascending=False)[:1].index\n",
    "plot_roc_curve_multiclass(y_test, y_test_pred, classes=None, #sample_weight=idx_test.astype(int).values,\n",
    "                          plot_classes=False,\n",
    "                          width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_pr_curve_multiclass(y_test, (y_test_pred>0.5).astype(int), classes=top_classes, #sample_weight=idx_test.astype(int).values, \n",
    "#                          plot_classes=False,\n",
    "#                           width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = largest_indices(y_test_pred.sum(axis=1), 50)\n",
    "cols = y_test.columns[largest_indices(y_test.iloc[idx].sum(0), 100)].sort_values()\n",
    "heatmap_compare(y_test.iloc[idx][cols], y_test_pred.iloc[idx][cols], \n",
    "                title=f\"Predictions on {n_classes} GO Terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = largest_indices(y_train_pred.sum(axis=1), 100)\n",
    "cols = y_train.columns[largest_indices(y_train.iloc[idx].sum(0), 100)].sort_values()\n",
    "heatmap_compare(y_train.iloc[idx][cols], y_train_pred.iloc[idx][cols], \n",
    "                title=f\"Predictions on {n_classes} GO Terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = UMAP(n_components=3).fit_transform(emb_all)\n",
    "pos = {idx_all.index[i]:pair for i, pair in enumerate(pos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist = idx_all.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ontology = GeneOntology()\n",
    "gene_ontology.filter_network(\"biological_process\")\n",
    "go_id_colors = gene_ontology.get_node_color(\"~/Bioinformatics_ExternalData/GeneOntology/go_colors_biological.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_color = network.get_labels_color(\"go_id\", go_id_colors, label_filter=set(gene_ontology.node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_viz3d(network.G_u, nodelist=nodelist, \n",
    "            pos=pos,\n",
    "            node_text=nodelist,# + \", \" +labels_terms.loc[nodelist],\n",
    "            node_symbol=idx_all.loc[nodelist],\n",
    "            node_color=labels_color.loc[nodelist],\n",
    "#             edge_label=\"database\",\n",
    "#             iterations=100,\n",
    "            max_edges=5000, showlegend=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moge.model.static_graph_embedding import ImportedGraphEmbedding\n",
    "from moge.evaluation.clustering import evaluate_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ige = ImportedGraphEmbedding(hparams[\"encoding_dim\"], \"MultiplexEmbedding\")\n",
    "ige.node_list = idx_all.index.to_list()\n",
    "ige._X = emb_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodelist = network.annotations[\"go_id\"][network.annotations[\"go_id\"].notnull()].index & pd.Index(ige.node_list) & y_train.index\n",
    "evaluate_clustering(ige, network.annotations, nodelist, node_label=\"go_id\", \n",
    "                    metrics=['homogeneity', 'completeness', 'nmi', 'ami'], max_clusters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance correlation analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings vs sequences\n",
    "print(distances_correlation(emb_train, network.annotations[[\"Transcript sequence\"]], index=y_train.index, n_nodes=200))\n",
    "print(distances_correlation(emb_test, network.annotations[[\"Transcript sequence\"]], index=y_test.index, n_nodes=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodings vs sequences\n",
    "print(distances_correlation(train_protein_encodings, network.annotations[[\"Transcript sequence\"]], index=y_train.index, n_nodes=100))\n",
    "print(distances_correlation(test_protein_encodings, network.annotations[[\"Transcript sequence\"]], index=y_test.index, n_nodes=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding vs Adj\n",
    "adj = nx.adjacency_matrix(network.G_u, nodelist=y_train.index).toarray()\n",
    "print(distances_correlation(emb_train, adj, index=y_train.index, n_nodes=200, verbose=False))\n",
    "adj = nx.adjacency_matrix(network.G_u, nodelist=y_test.index).toarray()\n",
    "print(distances_correlation(emb_test, adj, index=y_test.index, n_nodes=200, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding vs Adj\n",
    "adj = nx.adjacency_matrix(network.G_u, nodelist=y_train.index).toarray()\n",
    "print(distances_correlation(train_protein_encodings, adj, index=y_train.index, n_nodes=200, verbose=False))\n",
    "adj = nx.adjacency_matrix(network.G_u, nodelist=y_test.index).toarray()\n",
    "print(distances_correlation(test_protein_encodings, adj, index=y_test.index, n_nodes=200, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
