{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\\\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "from TCGAMultiOmics.multiomics import MultiOmicsData\n",
    "from moge.network.heterogeneous_network import HeterogeneousNetwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import network from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# WRITE\n",
    "# with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0.pickle', 'wb') as file:\n",
    "#     pickle.dump(network, file)\n",
    "\n",
    "# READ\n",
    "with open('moge/data/lncRNA_miRNA_mRNA/lncRNA-miRNA-mRNA_network_new.pickle', 'rb') as file:\n",
    "# with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_biogrid.pickle', 'rb') as file:\n",
    "    network = pickle.load(file)\n",
    "#     network.remove_extra_nodes()\n",
    "#     network.node_list = network.all_nodes\n",
    "#     node_list = network.node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u,v,d in network.G.edges(data=True):\n",
    "    if d[\"type\"] == 'u_n':\n",
    "        d['weight']+=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ edgelists\n",
    "# with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0_test_edges.pickle', 'rb') as file:\n",
    "#     test_edges_dict = pickle.load(file)\n",
    "    \n",
    "# with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0_val_edges.pickle', 'rb') as file:\n",
    "#     val_edges_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = network.multi_omics_data.load_data(modalities=[\"MIR\", \"GE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.multi_omics_data.external_data_path = \"/home/jonny/PycharmProjects/Bioinformatics_ExternalData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[\"MIR\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Source Target Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, Lambda, Dot, Dense, Flatten, MaxPooling1D, Lambda, Convolution1D, Layer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.metrics import kullback_leibler_divergence, binary_crossentropy, binary_accuracy\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "def W_init(shape, name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = np.random.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape, name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=np.random.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.reset_default_graph()\n",
    "# sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT PARAMETERS\n",
    "max_length = 500\n",
    "input_shape = (None, 6)\n",
    "batch_size = 512\n",
    "\n",
    "_d = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "E_ij = Input(batch_shape=(batch_size, 1), name=\"E_ij\")\n",
    "input_seq_i = Input(batch_shape=(batch_size, *input_shape), name=\"input_seq_i\")\n",
    "input_seq_j = Input(batch_shape=(batch_size, *input_shape), name=\"input_seq_j\")\n",
    "is_directed = Input(batch_shape=(batch_size, 1), dtype=tf.bool, name=\"is_directed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance functions\n",
    "def euclidean_distance(inputs):\n",
    "    x, y, _ = inputs\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def switch_emb(inputs):\n",
    "    emb_i, emb_j, is_directed = inputs\n",
    "    return K.switch(is_directed, \n",
    "                    [emb_i[:, 0:int(_d/2)] - emb_j[:, int(_d/2):_d]], [emb_i, emb_j]) \n",
    "\n",
    "def st_euclidean_distance(inputs):\n",
    "    emb_i, emb_j, is_directed = inputs\n",
    "    sum_directed = K.sum(K.square(emb_i[:, 0:int(_d/2)] - emb_j[:, int(_d/2):_d]), axis=1, keepdims=True)\n",
    "    sum_undirected = K.sum(K.square(emb_i - emb_j), axis=1, keepdims=True)\n",
    "    sum_switch = K.switch(is_directed, sum_directed, sum_undirected)\n",
    "    return K.sqrt(K.maximum(sum_switch, K.epsilon()))\n",
    "\n",
    "def st_embedding_probability(inputs):\n",
    "    emb_i, emb_j, is_directed = inputs\n",
    "    dot_directed = Dot(axes=1)([emb_i[:, 0:int(_d/2)], emb_j[:, int(_d/2):_d]])\n",
    "    dot_undirected = Dot(axes=1)([emb_i, emb_j])\n",
    "    return K.switch(is_directed, K.sigmoid(dot_directed), K.sigmoid(dot_undirected))\n",
    "\n",
    "def st_embedding_probability_w_dense(inputs):\n",
    "    emb_i, emb_j, is_directed = inputs\n",
    "    directed = Dense(1, activation='sigmoid')(Dot(axes=1)([emb_i[:, 0:int(_d/2)], emb_j[:, int(_d/2):_d]]))\n",
    "    undirected = Dense(1, activation='sigmoid')(Dot(axes=1)([emb_i, emb_j]))\n",
    "    return K.switch(is_directed, directed, undirected)\n",
    "\n",
    "def st_l1_distance(inputs):\n",
    "    emb_i, emb_j, is_directed = inputs\n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    directed_distance = Dense(1, activation='sigmoid')(L1_layer([emb_i[:, 0:int(_d/2)], emb_j[:, int(_d/2):_d]]))\n",
    "    undirected_distance = Dense(1, activation='sigmoid')(L1_layer([emb_i, emb_j]))\n",
    "    \n",
    "    return K.switch(is_directed, directed_distance, undirected_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    \"\"\" Base network to be shared (eq. to feature extraction).\n",
    "    \"\"\"\n",
    "    input = Input(shape=input_shape)\n",
    "#     x = Flatten()(input)\n",
    "    x = Convolution1D(filters=320, kernel_size=26, input_shape=input_shape, activation='relu')(input)\n",
    "    print(\"conv1d\", x)\n",
    "    x = MaxPooling1D(pool_size=13, strides=13)(x) # Similar to DanQ Model\n",
    "    print(\"max pooling\", x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(320, return_sequences=False, return_state=False))(x)\n",
    "    print(\"brnn\", x)\n",
    "    x = Dropout(0.5)(x)\n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "#     print(\"GAP pooling\", x)\n",
    "    \n",
    "    x = Dense(75*640, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(925, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(_d, activation='sigmoid')(x) # Embedding space\n",
    "    return Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    ''' Contrastive loss from Hadsell-et-al.'06 \n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1.0\n",
    "    return K.mean(y_true * K.square(y_pred) +\n",
    "                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def regularized_cross_entropy(y_true, y_pred):\n",
    "    return K.mean(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "def kl_loss(y_true, y_pred):\n",
    "    return -K.mean(y_true * K.log(y_pred))\n",
    "\n",
    "# Metrics\n",
    "def accuracy(y_true, y_pred):\n",
    "    ''' Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d Tensor(\"conv1d_1/Relu:0\", shape=(?, 475, 320), dtype=float32)\n",
      "max pooling Tensor(\"max_pooling1d_1/Squeeze:0\", shape=(?, 36, 320), dtype=float32)\n",
      "brnn Tensor(\"bidirectional_1/concat:0\", shape=(?, 640), dtype=float32)\n",
      "lstm_network.input Tensor(\"input_1:0\", shape=(?, 500, 6), dtype=float32)\n",
      "lstm_network.output Tensor(\"dense_3/Sigmoid:0\", shape=(?, 128), dtype=float32)\n",
      "input_seq_i Tensor(\"input_seq_i:0\", shape=(512, 500, 6), dtype=float32)\n",
      "input_seq_j Tensor(\"input_seq_j:0\", shape=(512, 500, 6), dtype=float32)\n",
      "encoded_i Tensor(\"model_1/dense_3/Sigmoid:0\", shape=(512, 128), dtype=float32) \n",
      "encoded_j Tensor(\"model_1_1/dense_3/Sigmoid:0\", shape=(512, 128), dtype=float32)\n",
      "distance Tensor(\"lambda_1/Sqrt:0\", shape=(512, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# build create_base_network to use in each siamese 'leg'\n",
    "lstm_network = create_base_network(input_shape=(max_length, 6))\n",
    "\n",
    "print(\"lstm_network.input\", lstm_network.input)\n",
    "print(\"lstm_network.output\", lstm_network.output)\n",
    "print(\"input_seq_i\", input_seq_i)\n",
    "print(\"input_seq_j\", input_seq_j)\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_i = lstm_network(input_seq_i)\n",
    "encoded_j = lstm_network(input_seq_j)\n",
    "print(\"encoded_i\", encoded_i, \"\\nencoded_j\", encoded_j)\n",
    "\n",
    "distance = Lambda(st_euclidean_distance)([encoded_i, encoded_j, is_directed])\n",
    "print(\"distance\", distance)\n",
    "\n",
    "siamese_net = Model(inputs=[input_seq_i, input_seq_j, is_directed], outputs=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76978653"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=contrastive_loss, \n",
    "                    optimizer=RMSprop(),\n",
    "                    metrics=[accuracy])\n",
    "\n",
    "siamese_net.count_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "# tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes info columns: Index(['Transcript sequence', 'Disease association'], dtype='object')\n",
      "Number of nodes without seq removed: 2572\n",
      "num_words: None {'A': 1, 'T': 2, 'G': 3, 'C': 4, 'U': 5}\n",
      "Ed_count: 446712 , Eu_count: 828043 , En_count: 733188\n",
      "Ens_count: 1816322\n",
      "X: [('input_seq_j', (512, 500, 6)), ('input_seq_i', (512, 500, 6)), ('is_directed', (512, 1))] \n",
      "y: (512, 1)\n"
     ]
    }
   ],
   "source": [
    "from moge.network.data_generator import DataGenerator\n",
    "\n",
    "generator = DataGenerator(network=network, get_training_data=False, negative_sampling_ratio=2.0,\n",
    "                          maxlen=max_length, padding='post', truncating=\"post\",\n",
    "                          batch_size=batch_size, dim=input_shape, \n",
    "                          shuffle=True, seed=0)\n",
    "\n",
    "X, y = generator.__getitem__(0)\n",
    "print(\"X:\", [(k, v.shape) for k, v in X.items()], \"\\ny:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3921/3921 [==============================] - 1576s 402ms/step - loss: 0.0671 - accuracy: 0.9260\n",
      "Epoch 2/10\n",
      "3921/3921 [==============================] - 1639s 418ms/step - loss: 0.0475 - accuracy: 0.9410\n",
      "Epoch 3/10\n",
      "2699/3921 [===================>..........] - ETA: 8:41 - loss: 0.0445 - accuracy: 0.9445"
     ]
    }
   ],
   "source": [
    "siamese_net.fit_generator(generator, epochs=10, use_multiprocessing=True, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "lstm_network.save(\"lstm_network_contrastive_st-eucl_128.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dense_3/Sigmoid:0' shape=(?, 128) dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_network.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No data provided for \"input_1\". Need data for each key in: ['input_1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/jonny_conda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             ]\n",
      "\u001b[0;32m~/.conda/envs/jonny_conda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a91495f5d3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# i += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jonny_conda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jonny_conda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jonny_conda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     75\u001b[0m             raise ValueError('No data provided for \"' + e.args[0] +\n\u001b[1;32m     76\u001b[0m                              \u001b[0;34m'\". Need data '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                              'for each key in: ' + str(names))\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No data provided for \"input_1\". Need data for each key in: ['input_1']"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "# i += 1\n",
    "X, y = generator.__getitem__(i)\n",
    "print(lstm_network.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"input_seq_i\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
