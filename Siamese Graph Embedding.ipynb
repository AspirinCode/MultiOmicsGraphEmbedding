{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\\\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "from TCGAMultiOmics.multiomics import MultiOmicsData\n",
    "from moge.network.heterogeneous_network import HeterogeneousNetwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import network from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# WRITE\n",
    "# with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0.pickle', 'wb') as file:\n",
    "#     pickle.dump(network, file)\n",
    "\n",
    "# READ\n",
    "with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0.pickle', 'rb') as file:\n",
    "# with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_biogrid.pickle', 'rb') as file:\n",
    "    network = pickle.load(file)\n",
    "    network.remove_extra_nodes()\n",
    "#     network.node_list = network.all_nodes\n",
    "#     node_list = network.node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ edgelists\n",
    "with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0_test_edges.pickle', 'rb') as file:\n",
    "    test_edges_dict = pickle.load(file)\n",
    "    \n",
    "with open('moge/data/lncRNA_miRNA_mRNA/miRNA-mRNA_network_test_05_val_01_seed_0_val_edges.pickle', 'rb') as file:\n",
    "    val_edges_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = network.multi_omics_data.load_data(modalities=[\"MIR\", \"GE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 1870)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"MIR\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/TCGAMultiOmics/genomic.py:291: FutureWarning: 'symbol' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  self.gene_info = self.gene_info.join(self.HGNC_lncrna_info.groupby(\"symbol\").first(), on=\"symbol\",\n",
      "/opt/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6336: FutureWarning: 'Gene Name' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  rsuffix=rsuffix, sort=sort)\n"
     ]
    }
   ],
   "source": [
    "transcripts = network.multi_omics_data.LNC.get_genes_info()[\"Transcript sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11769.000000\n",
       "mean       882.826493\n",
       "std        988.346422\n",
       "min         60.000000\n",
       "25%        447.000000\n",
       "50%        568.000000\n",
       "75%        833.000000\n",
       "max      37852.000000\n",
       "Name: Transcript sequence, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[transcripts.notna()].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Source Target Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, Lambda, Subtract, Dense, Flatten, MaxPooling1D\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = np.random.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=np.random.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_features_size = X[\"MIR\"].shape[0]\n",
    "input_shape = (None, 4)\n",
    "_d = 128\n",
    "n_nodes = len(network.node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    E_ij = tf.placeholder(tf.float32, shape=(1,), name=\"E_ij\")\n",
    "#     input_i = tf.placeholder(tf.float32, shape=input_shape, name=\"input_i\")\n",
    "#     input_j = tf.placeholder(tf.float32, shape=input_shape, name=\"input_j\")\n",
    "    input_seq_i = Input(shape=input_shape, name=\"input_i\")\n",
    "    input_seq_j = Input(shape=input_shape, name=\"input_j\")\n",
    "    is_directed = tf.placeholder(tf.bool, name=\"is_directed\")\n",
    "    i = tf.Variable(int, name=\"i\", trainable=False)\n",
    "    j = tf.Variable(int, name=\"j\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_10/Sigmoid:0' shape=(?, ?, 1) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build convnet to use in each siamese 'leg'\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, return_sequences=True, input_shape=input_shape))\n",
    "lstm.add(LSTM(8, return_sequences=True))\n",
    "lstm.add(TimeDistributed(Dense(_d, activation='sigmoid')))\n",
    "\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = lstm(input_seq_i)\n",
    "encoded_r = lstm(input_seq_j)\n",
    "print(encoded_l, encoded_r)\n",
    "#merge two encoded inputs with the l1 distance between them\n",
    "subtracted = Subtract()([encoded_l, encoded_r])\n",
    "both = K.abs(subtracted)\n",
    "print(both)\n",
    "prediction = Dense(1, activation='sigmoid', bias_initializer=b_init)(both)\n",
    "siamese_net = Model(inputs=[input_seq_i, input_seq_j], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        sequence_length = np.random.randint(10, 100)\n",
    "        x_train = np.random.random((1, sequence_length, 4))\n",
    "        # y_train will depend on past 5 timesteps of x\n",
    "        y_train = x_train[:, :, 0]\n",
    "        for i in range(1, 2):\n",
    "            y_train[:, i:] += x_train[:, :-i, i]\n",
    "        y_train = to_categorical(y_train > 2.5)\n",
    "        yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 88, 4) (1, 88, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = next(train_generator())\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'siamese_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-cbfca85a87e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.00006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'siamese_net' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.00006)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('siamese'):\n",
    "    siamese = Dense(128, activation='relu')(N_i)\n",
    "    siamese = Dense(_d, activation='relu')(siamese)\n",
    "    \n",
    "    emb_c_i = siamese(N_i)\n",
    "    emb_c_i = siamese(N_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('embedding'):\n",
    "    emb_s = tf.Variable(initial_value=tf.random_uniform([n_nodes, int(_d/2)], -1, 1),\n",
    "                        validate_shape=True, dtype=tf.float32,\n",
    "                        name=\"emb_t\", trainable=True)\n",
    "\n",
    "    emb_t = tf.Variable(initial_value=tf.random_uniform([n_nodes, int(_d/2)], -1, 1),\n",
    "                        validate_shape=True, dtype=tf.float32,\n",
    "                        name=\"emb_s\", trainable=True)\n",
    "\n",
    "    emb_c = tf.concat([emb_s, emb_t], axis=1, name=\"emb_concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_s = emb_s[i].assign(tf.reshape(siamese[:, 0 : int(_d/2)], [-1]))\n",
    "emb_t = emb_t[i].assign(tf.reshape(siamese[:, int(_d/2) : _d], [-1]))\n",
    "\n",
    "# with tf.control_dependencies([emb_s[i].assign(emb_c_i)]):\n",
    "#     emb_s = tf.identity(emb_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.as_default()\n",
    "K.set_session(sess)\n",
    "session.run(init_op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
